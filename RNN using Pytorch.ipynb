{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d57077f-0e57-4d9a-9e8d-13df5006f758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who painted the Mona Lisa?</td>\n",
       "      <td>Leonardo-da-Vinci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the square root of 64?</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the chemical symbol for gold?</td>\n",
       "      <td>Au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Which year did World War II end?</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the longest river in the world?</td>\n",
       "      <td>Nile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question             answer\n",
       "0                   What is the capital of France?              Paris\n",
       "1                  What is the capital of Germany?             Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?         Harper-Lee\n",
       "3  What is the largest planet in our solar system?            Jupiter\n",
       "4   What is the boiling point of water in Celsius?                100\n",
       "5                       Who painted the Mona Lisa?  Leonardo-da-Vinci\n",
       "6                   What is the square root of 64?                  8\n",
       "7            What is the chemical symbol for gold?                 Au\n",
       "8                 Which year did World War II end?               1945\n",
       "9          What is the longest river in the world?               Nile"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('100_Unique_QA_Dataset.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7786a5d-eb0c-4bc0-8137-40ed1a8b6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the english text into numbers\n",
    "\n",
    "# tokenize -> seperating each and ever word ,so that we can allot tokens\n",
    "def tokenize(text):\n",
    "    text = text.lower()  # converted all the alphabets into lower letters\n",
    "    text = text.replace('?' , '') # replaced ? with nothing\n",
    "    text = text.replace(\" ' \" , \"\") # replaced ' with nothing\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89554f49-e42e-43bb-b577-943be4f1c88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'capital', 'of', 'germany']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275d2ccb-b02f-49c6-9c7e-83e31dcf2c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary -> how many unique words are there is our dataset , and providing each unique word \"index\"\n",
    "vocab = {'<UNK>' :0} # unknown toekn -> if in future there are some words which are not in our dataset , we will replace the respective word with the unknown token\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81dab5d5-4a1e-4a35-bdb0-66c48d325877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row): \n",
    "    print(row['question'] , row['answer'] )  # untokized question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c98be6f2-f2ef-4664-92f6-3ccbcc198737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row):\n",
    "  tokenized_question = tokenize(row['question']) # tokenized question stored in tokenized_question\n",
    "  tokenized_answer = tokenize(row['answer']) # tokenized answer stored in tokenized_answer\n",
    "\n",
    "  merged_tokens = tokenized_question + tokenized_answer # merged the two ,  in a single list all will get printed\n",
    "\n",
    "  for token in merged_tokens:\n",
    "\n",
    "    if token not in vocab:      # checking that is the token available in the dictionary \n",
    "      vocab[token] = len(vocab)  # if not available , we will add the vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e489b6e6-1d4f-4024-b20a-d3e4443fce0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(build_vocab , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dda5770c-b94e-4856-83e1-49d3fedfe55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " \"'to\": 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " \"mockingbird'\": 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " \"'1984'\": 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " \"'starry\": 130,\n",
       " \"night'\": 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'to': 143,\n",
       " 'step': 144,\n",
       " 'moon': 145,\n",
       " 'armstrong': 146,\n",
       " 'main': 147,\n",
       " 'ingredient': 148,\n",
       " 'guacamole': 149,\n",
       " 'avocado': 150,\n",
       " 'sides': 151,\n",
       " 'does': 152,\n",
       " 'hexagon': 153,\n",
       " 'have': 154,\n",
       " '6': 155,\n",
       " 'china': 156,\n",
       " 'yuan': 157,\n",
       " \"'pride\": 158,\n",
       " 'and': 159,\n",
       " \"prejudice'\": 160,\n",
       " 'jane-austen': 161,\n",
       " 'iron': 162,\n",
       " 'fe': 163,\n",
       " 'hardest': 164,\n",
       " 'natural': 165,\n",
       " 'substance': 166,\n",
       " 'diamond': 167,\n",
       " 'continent': 168,\n",
       " 'by': 169,\n",
       " 'area': 170,\n",
       " 'asia': 171,\n",
       " 'president': 172,\n",
       " 'states': 173,\n",
       " 'george-washington': 174,\n",
       " 'bird': 175,\n",
       " 'its': 176,\n",
       " 'ability': 177,\n",
       " 'mimic': 178,\n",
       " 'sounds': 179,\n",
       " 'parrot': 180,\n",
       " 'longest-running': 181,\n",
       " 'animated': 182,\n",
       " 'tv': 183,\n",
       " 'show': 184,\n",
       " 'simpsons': 185,\n",
       " 'vaticancity': 186,\n",
       " 'most': 187,\n",
       " 'moons': 188,\n",
       " 'saturn': 189,\n",
       " \"'romeo\": 190,\n",
       " \"juliet'\": 191,\n",
       " 'shakespeare': 192,\n",
       " \"earth's\": 193,\n",
       " 'atmosphere': 194,\n",
       " 'nitrogen': 195,\n",
       " 'bones': 196,\n",
       " 'adult': 197,\n",
       " 'human': 198,\n",
       " 'body': 199,\n",
       " '206': 200,\n",
       " 'metal': 201,\n",
       " 'liquid': 202,\n",
       " 'at': 203,\n",
       " 'room': 204,\n",
       " 'temperature': 205,\n",
       " 'mercury': 206,\n",
       " 'russia': 207,\n",
       " 'moscow': 208,\n",
       " 'electricity': 209,\n",
       " 'benjamin-franklin': 210,\n",
       " 'second-largest': 211,\n",
       " 'land': 212,\n",
       " 'color': 213,\n",
       " 'ripe': 214,\n",
       " 'banana': 215,\n",
       " 'yellow': 216,\n",
       " 'month': 217,\n",
       " '28': 218,\n",
       " 'days': 219,\n",
       " 'common': 220,\n",
       " 'february': 221,\n",
       " 'study': 222,\n",
       " 'living': 223,\n",
       " 'organisms': 224,\n",
       " 'called': 225,\n",
       " 'biology': 226,\n",
       " 'home': 227,\n",
       " 'great': 228,\n",
       " 'wall': 229,\n",
       " 'bees': 230,\n",
       " 'collect': 231,\n",
       " 'from': 232,\n",
       " 'flowers': 233,\n",
       " 'nectar': 234,\n",
       " 'opposite': 235,\n",
       " \"'day'\": 236,\n",
       " 'night': 237,\n",
       " 'south': 238,\n",
       " 'korea': 239,\n",
       " 'seoul': 240,\n",
       " 'bulb': 241,\n",
       " 'edison': 242,\n",
       " 'humans': 243,\n",
       " 'breathe': 244,\n",
       " 'survival': 245,\n",
       " 'oxygen': 246,\n",
       " '144': 247,\n",
       " '12': 248,\n",
       " 'pyramids': 249,\n",
       " 'giza': 250,\n",
       " 'egypt': 251,\n",
       " 'sea': 252,\n",
       " 'creature': 253,\n",
       " 'eight': 254,\n",
       " 'arms': 255,\n",
       " 'octopus': 256,\n",
       " 'holiday': 257,\n",
       " 'celebrated': 258,\n",
       " 'december': 259,\n",
       " '25': 260,\n",
       " 'christmas': 261,\n",
       " 'yen': 262,\n",
       " 'legs': 263,\n",
       " 'spider': 264,\n",
       " 'sport': 265,\n",
       " 'uses': 266,\n",
       " 'net,': 267,\n",
       " 'ball,': 268,\n",
       " 'hoop': 269,\n",
       " 'basketball': 270,\n",
       " 'kangaroos': 271,\n",
       " 'female': 272,\n",
       " 'minister': 273,\n",
       " 'uk': 274,\n",
       " 'margaretthatcher': 275,\n",
       " 'fastest': 276,\n",
       " 'animal': 277,\n",
       " 'cheetah': 278,\n",
       " 'periodic': 279,\n",
       " 'table': 280,\n",
       " 'spain': 281,\n",
       " 'madrid': 282,\n",
       " 'closest': 283,\n",
       " 'sun': 284,\n",
       " 'father': 285,\n",
       " 'computers': 286,\n",
       " 'charlesbabbage': 287,\n",
       " 'mexico': 288,\n",
       " 'mexicocity': 289,\n",
       " 'colors': 290,\n",
       " 'rainbow': 291,\n",
       " 'musical': 292,\n",
       " 'instrument': 293,\n",
       " 'black': 294,\n",
       " 'white': 295,\n",
       " 'keys': 296,\n",
       " 'piano': 297,\n",
       " 'americas': 298,\n",
       " '1492': 299,\n",
       " 'christophercolumbus': 300,\n",
       " 'disney': 301,\n",
       " 'character': 302,\n",
       " 'long': 303,\n",
       " 'nose': 304,\n",
       " 'grows': 305,\n",
       " 'it': 306,\n",
       " 'when': 307,\n",
       " 'lying': 308,\n",
       " 'pinocchio': 309,\n",
       " 'directed': 310,\n",
       " 'movie': 311,\n",
       " \"'titanic'\": 312,\n",
       " 'jamescameron': 313,\n",
       " 'superhero': 314,\n",
       " 'also': 315,\n",
       " 'dark': 316,\n",
       " 'knight': 317,\n",
       " 'batman': 318,\n",
       " 'brasilia': 319,\n",
       " 'fruit': 320,\n",
       " 'king': 321,\n",
       " 'fruits': 322,\n",
       " 'mango': 323,\n",
       " 'eiffel': 324,\n",
       " 'tower': 325}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eea0cebe-08e9-4696-8b11-09284d9aee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb1dd661-39a0-4337-b459-8a5e9b416506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words  tp numerical indices -> eg - \"What\" , will be replaced by its token number\n",
    "def text_to_indices(text, vocab):\n",
    "\n",
    "  indexed_text = []\n",
    "\n",
    "  for token in tokenize(text):\n",
    "\n",
    "    if token in vocab:\n",
    "      indexed_text.append(vocab[token])\n",
    "    else:\n",
    "      indexed_text.append(vocab['<UNK>'])\n",
    "\n",
    "  return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "839aebcb-744d-41b4-838c-fafdf4d8129b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 2, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(\"Who is Sarvambh\" , vocab) # who = 10 , is = 2 , sarvambh -> is unknown as it is not present in the dictionary hence replaced by unknown - 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "959eb77f-b33e-42e8-823b-8c26cbc0a8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " \"'to\": 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " \"mockingbird'\": 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " \"'1984'\": 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " \"'starry\": 130,\n",
       " \"night'\": 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'to': 143,\n",
       " 'step': 144,\n",
       " 'moon': 145,\n",
       " 'armstrong': 146,\n",
       " 'main': 147,\n",
       " 'ingredient': 148,\n",
       " 'guacamole': 149,\n",
       " 'avocado': 150,\n",
       " 'sides': 151,\n",
       " 'does': 152,\n",
       " 'hexagon': 153,\n",
       " 'have': 154,\n",
       " '6': 155,\n",
       " 'china': 156,\n",
       " 'yuan': 157,\n",
       " \"'pride\": 158,\n",
       " 'and': 159,\n",
       " \"prejudice'\": 160,\n",
       " 'jane-austen': 161,\n",
       " 'iron': 162,\n",
       " 'fe': 163,\n",
       " 'hardest': 164,\n",
       " 'natural': 165,\n",
       " 'substance': 166,\n",
       " 'diamond': 167,\n",
       " 'continent': 168,\n",
       " 'by': 169,\n",
       " 'area': 170,\n",
       " 'asia': 171,\n",
       " 'president': 172,\n",
       " 'states': 173,\n",
       " 'george-washington': 174,\n",
       " 'bird': 175,\n",
       " 'its': 176,\n",
       " 'ability': 177,\n",
       " 'mimic': 178,\n",
       " 'sounds': 179,\n",
       " 'parrot': 180,\n",
       " 'longest-running': 181,\n",
       " 'animated': 182,\n",
       " 'tv': 183,\n",
       " 'show': 184,\n",
       " 'simpsons': 185,\n",
       " 'vaticancity': 186,\n",
       " 'most': 187,\n",
       " 'moons': 188,\n",
       " 'saturn': 189,\n",
       " \"'romeo\": 190,\n",
       " \"juliet'\": 191,\n",
       " 'shakespeare': 192,\n",
       " \"earth's\": 193,\n",
       " 'atmosphere': 194,\n",
       " 'nitrogen': 195,\n",
       " 'bones': 196,\n",
       " 'adult': 197,\n",
       " 'human': 198,\n",
       " 'body': 199,\n",
       " '206': 200,\n",
       " 'metal': 201,\n",
       " 'liquid': 202,\n",
       " 'at': 203,\n",
       " 'room': 204,\n",
       " 'temperature': 205,\n",
       " 'mercury': 206,\n",
       " 'russia': 207,\n",
       " 'moscow': 208,\n",
       " 'electricity': 209,\n",
       " 'benjamin-franklin': 210,\n",
       " 'second-largest': 211,\n",
       " 'land': 212,\n",
       " 'color': 213,\n",
       " 'ripe': 214,\n",
       " 'banana': 215,\n",
       " 'yellow': 216,\n",
       " 'month': 217,\n",
       " '28': 218,\n",
       " 'days': 219,\n",
       " 'common': 220,\n",
       " 'february': 221,\n",
       " 'study': 222,\n",
       " 'living': 223,\n",
       " 'organisms': 224,\n",
       " 'called': 225,\n",
       " 'biology': 226,\n",
       " 'home': 227,\n",
       " 'great': 228,\n",
       " 'wall': 229,\n",
       " 'bees': 230,\n",
       " 'collect': 231,\n",
       " 'from': 232,\n",
       " 'flowers': 233,\n",
       " 'nectar': 234,\n",
       " 'opposite': 235,\n",
       " \"'day'\": 236,\n",
       " 'night': 237,\n",
       " 'south': 238,\n",
       " 'korea': 239,\n",
       " 'seoul': 240,\n",
       " 'bulb': 241,\n",
       " 'edison': 242,\n",
       " 'humans': 243,\n",
       " 'breathe': 244,\n",
       " 'survival': 245,\n",
       " 'oxygen': 246,\n",
       " '144': 247,\n",
       " '12': 248,\n",
       " 'pyramids': 249,\n",
       " 'giza': 250,\n",
       " 'egypt': 251,\n",
       " 'sea': 252,\n",
       " 'creature': 253,\n",
       " 'eight': 254,\n",
       " 'arms': 255,\n",
       " 'octopus': 256,\n",
       " 'holiday': 257,\n",
       " 'celebrated': 258,\n",
       " 'december': 259,\n",
       " '25': 260,\n",
       " 'christmas': 261,\n",
       " 'yen': 262,\n",
       " 'legs': 263,\n",
       " 'spider': 264,\n",
       " 'sport': 265,\n",
       " 'uses': 266,\n",
       " 'net,': 267,\n",
       " 'ball,': 268,\n",
       " 'hoop': 269,\n",
       " 'basketball': 270,\n",
       " 'kangaroos': 271,\n",
       " 'female': 272,\n",
       " 'minister': 273,\n",
       " 'uk': 274,\n",
       " 'margaretthatcher': 275,\n",
       " 'fastest': 276,\n",
       " 'animal': 277,\n",
       " 'cheetah': 278,\n",
       " 'periodic': 279,\n",
       " 'table': 280,\n",
       " 'spain': 281,\n",
       " 'madrid': 282,\n",
       " 'closest': 283,\n",
       " 'sun': 284,\n",
       " 'father': 285,\n",
       " 'computers': 286,\n",
       " 'charlesbabbage': 287,\n",
       " 'mexico': 288,\n",
       " 'mexicocity': 289,\n",
       " 'colors': 290,\n",
       " 'rainbow': 291,\n",
       " 'musical': 292,\n",
       " 'instrument': 293,\n",
       " 'black': 294,\n",
       " 'white': 295,\n",
       " 'keys': 296,\n",
       " 'piano': 297,\n",
       " 'americas': 298,\n",
       " '1492': 299,\n",
       " 'christophercolumbus': 300,\n",
       " 'disney': 301,\n",
       " 'character': 302,\n",
       " 'long': 303,\n",
       " 'nose': 304,\n",
       " 'grows': 305,\n",
       " 'it': 306,\n",
       " 'when': 307,\n",
       " 'lying': 308,\n",
       " 'pinocchio': 309,\n",
       " 'directed': 310,\n",
       " 'movie': 311,\n",
       " \"'titanic'\": 312,\n",
       " 'jamescameron': 313,\n",
       " 'superhero': 314,\n",
       " 'also': 315,\n",
       " 'dark': 316,\n",
       " 'knight': 317,\n",
       " 'batman': 318,\n",
       " 'brasilia': 319,\n",
       " 'fruit': 320,\n",
       " 'king': 321,\n",
       " 'fruits': 322,\n",
       " 'mango': 323,\n",
       " 'eiffel': 324,\n",
       " 'tower': 325}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311308c-86c8-4bba-a902-041b74bf408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to go to our each row and questions and answer , and convert it into numerical indices\n",
    "# we will taek hellp of data set and data loaders classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e04c438-d354-43c8-9abc-66e5bc77b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset , DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebaa64e8-b439-4139-bbc1-24312016cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "\n",
    "  def __init__(self, df, vocab):\n",
    "    self.df = df\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.df.shape[0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "\n",
    "    numerical_question = text_to_indices(self.df.iloc[index]['question'], self.vocab) # converted the whole question text into respecctive indices and stored it in numerical_question\n",
    "    numerical_answer = text_to_indices(self.df.iloc[index]['answer'], self.vocab) # converted the whole answer text into respective indices adn stored it in numerical_answer\n",
    "\n",
    "    return torch.tensor(numerical_question), torch.tensor(numerical_answer) # converetd the respectiev indices into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b138a263-effe-4857-89a8-528a59578715",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QADataset(df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d75da26-2ef2-4b38-9e55-506d680159a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size = 1 , shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7af03f8d-405f-4029-8c7f-7833f5e1cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,   2,   3,  33,  34,   5, 247]]) tensor([[248]])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([[186]])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([[32]])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([[36]])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([[91]])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([[28]])\n",
      "tensor([[ 10,  11, 158, 159, 160]]) tensor([[161]])\n",
      "tensor([[ 78,  79, 196,  81,  19,   3, 197, 198, 199]]) tensor([[200]])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([[262]])\n",
      "tensor([[ 42, 265, 266,  14, 267, 268, 159, 269]]) tensor([[270]])\n",
      "tensor([[ 42, 292, 293, 118, 294, 159, 295, 296]]) tensor([[297]])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 162]]) tensor([[163]])\n",
      "tensor([[ 42, 217, 118, 218, 219,  19,  14, 220,  43]]) tensor([[221]])\n",
      "tensor([[  1,   2,   3,   4,   5, 288]]) tensor([[289]])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([[85]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([[100]])\n",
      "tensor([[ 42,  18,   2,   3, 283, 143,   3, 284]]) tensor([[206]])\n",
      "tensor([[ 42, 168,   2,   3,  17, 169, 170]]) tensor([[171]])\n",
      "tensor([[ 10,  11, 190, 159, 191]]) tensor([[192]])\n",
      "tensor([[ 42, 314,   2, 315,  62,  63,   3, 316, 317]]) tensor([[318]])\n",
      "tensor([[  1,   2,   3,   4,   5, 207]]) tensor([[208]])\n",
      "tensor([[ 42,  86,  87, 243, 244,  19,  39, 245]]) tensor([[246]])\n",
      "tensor([[ 10,  75, 209]]) tensor([[210]])\n",
      "tensor([[ 42, 175,   2,  62,  39, 176, 177, 143, 178, 179]]) tensor([[180]])\n",
      "tensor([[  1,   2,   3, 235,   5, 236]]) tensor([[237]])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([[9]])\n",
      "tensor([[ 10,  75,   3, 298,  19, 299]]) tensor([[300]])\n",
      "tensor([[  1,  87, 230, 231, 232, 233]]) tensor([[234]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([[53]])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 279, 280]]) tensor([[121]])\n",
      "tensor([[ 42, 137, 118,   3, 249,   5, 250]]) tensor([[251]])\n",
      "tensor([[ 42, 252, 253, 118, 254, 255]]) tensor([[256]])\n",
      "tensor([[  1,   2,   3, 147, 148,  19, 149]]) tensor([[150]])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([[134]])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([[106]])\n",
      "tensor([[  1,   2,   3, 147,  86,  19, 193, 194]]) tensor([[195]])\n",
      "tensor([[ 10,  96,   3, 104, 241]]) tensor([[242]])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([[52]])\n",
      "tensor([[  1,   2,   3, 181, 182, 183, 184]]) tensor([[185]])\n",
      "tensor([[10, 96,  3, 97]]) tensor([[98]])\n",
      "tensor([[ 42, 301, 302, 118,  14, 303, 304, 159, 305, 306, 307, 308]]) tensor([[309]])\n",
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([[121]])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([[23]])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([[36]])\n",
      "tensor([[  1,   2,   3, 213,   5,  14, 214, 215]]) tensor([[216]])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([[319]])\n",
      "tensor([[ 10, 310,   3, 311, 312]]) tensor([[313]])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([[116]])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([[41]])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([[49]])\n",
      "tensor([[ 10, 140,   3, 141, 172,   5,   3,  70, 173]]) tensor([[174]])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([[110]])\n",
      "tensor([[  1,   2,   3,   4,   5, 281]]) tensor([[282]])\n",
      "tensor([[ 10,  75, 111]]) tensor([[112]])\n",
      "tensor([[ 42,   2,   3, 211, 137, 169, 212, 170]]) tensor([[113]])\n",
      "tensor([[ 78,  79, 151, 152,  14, 153, 154]]) tensor([[155]])\n",
      "tensor([[  1,   2,   3,  69,   5, 156]]) tensor([[157]])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([[102]])\n",
      "tensor([[  1,   2,   3, 164, 165, 166,  83,  84]]) tensor([[167]])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([[68]])\n",
      "tensor([[ 10, 140,   3, 141, 272,  93, 273,   5,   3, 274]]) tensor([[275]])\n",
      "tensor([[ 42, 257,   2, 258,  83, 259, 260]]) tensor([[261]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([[74]])\n",
      "tensor([[10, 75, 76]]) tensor([[77]])\n",
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([[65]])\n",
      "tensor([[ 10, 140,   3, 141, 142, 143, 144,  83,   3, 145]]) tensor([[146]])\n",
      "tensor([[ 42, 137,   2, 227, 143,   3, 228, 229]]) tensor([[156]])\n",
      "tensor([[ 10,   2,  62,  63,   3, 285,   5, 286]]) tensor([[287]])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([[61]])\n",
      "tensor([[ 78,  79, 290,  81,  19,  14, 291]]) tensor([[85]])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([[128]])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([[136]])\n",
      "tensor([[ 78,  79, 263, 152,  14, 264, 154]]) tensor([[36]])\n",
      "tensor([[ 42, 320,   2,  62,  63,   3, 321,   5, 322]]) tensor([[323]])\n",
      "tensor([[ 42,   2,   3, 276, 212, 277]]) tensor([[278]])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 324, 325]]) tensor([[6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([[7]])\n",
      "tensor([[  1,   2,   3,   4,   5, 238, 239]]) tensor([[240]])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([[132]])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([[114]])\n",
      "tensor([[  1,   2,   3, 222,   5, 223, 224, 225]]) tensor([[226]])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([[72]])\n",
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([[16]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 176, 271]]) tensor([[99]])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([[58]])\n",
      "tensor([[ 42,  18, 118,   3, 187, 188]]) tensor([[189]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([[54]])\n",
      "tensor([[ 42, 201,   2,  14, 202, 203, 204, 205]]) tensor([[206]])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([[124]])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([[95]])\n"
     ]
    }
   ],
   "source": [
    "for question , answer in dataloader:\n",
    "    print(question , answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df192c6-a245-4097-8174-e63d65a83282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buiding the RNN ARCHITECTURE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58fb1dd4-025c-415e-9af1-2a50f18646ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38db336f-e5af-4b81-b3ed-9c252da7817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim=50) # converting your each word into a 50 dimensional layer  \n",
    "    self.rnn = nn.RNN(50, 64, batch_first=True)\n",
    "    self.fc = nn.Linear(64, vocab_size)\n",
    "\n",
    "  def forward(self, question):\n",
    "    embedded_question = self.embedding(question)\n",
    "    hidden, final = self.rnn(embedded_question)\n",
    "    output = self.fc(final.squeeze(0))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c1ae253-1570-4fa0-bdfc-8237ab5848b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5198fc63-51fc-4371-bc58-2549235db306",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Embedding(324 , embedding_dim = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bac08fc0-95c0-420e-a922-39378758a76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2503e+00, -1.0524e+00,  5.6339e-01, -2.4573e-01, -1.1826e+00,\n",
       "         -7.0119e-02,  8.4112e-01,  1.3265e+00,  1.7870e-01, -3.3997e-01,\n",
       "          1.9113e+00, -3.1746e-01, -3.1415e+00, -8.2214e-01,  2.1212e-01,\n",
       "         -1.0891e+00,  8.8661e-01,  1.6221e+00, -4.2221e-01,  9.7907e-01,\n",
       "          1.8672e+00,  4.1413e-04, -5.6157e-01, -1.2633e+00,  2.7772e-01,\n",
       "          9.5701e-01, -9.0328e-01,  1.6230e-01, -3.3102e-01, -1.1106e+00,\n",
       "          4.4981e-02,  1.2929e+00,  1.9248e+00,  7.9192e-01,  1.2147e+00,\n",
       "         -3.7983e-01,  1.8594e+00, -1.2367e-01, -1.3795e+00, -8.0557e-02,\n",
       "         -1.1671e+00, -1.4057e+00,  3.5313e-02, -1.3959e+00,  3.9498e-01,\n",
       "         -6.3847e-01,  2.0553e-01,  1.0085e+00, -2.8827e-01,  8.7322e-01],\n",
       "        [-7.2541e-01,  7.8185e-01,  4.2582e-01, -3.5482e-01, -1.4095e-01,\n",
       "         -1.0340e-01, -1.7049e+00, -1.4701e+00,  1.2551e+00,  2.0202e+00,\n",
       "          8.1504e-01, -1.0414e+00, -1.9095e+00, -2.3607e+00, -2.4665e-01,\n",
       "         -4.7142e-01,  8.1917e-01, -9.6144e-01, -1.1680e+00,  7.5940e-01,\n",
       "         -9.7052e-01,  1.2056e-01, -1.0663e+00,  6.6591e-01,  1.1103e-01,\n",
       "          8.2518e-01,  2.2369e-01, -8.2993e-01, -2.8473e-01, -1.4044e+00,\n",
       "         -6.2424e-01, -2.7509e-01,  7.5155e-01, -1.0770e+00,  1.2649e-03,\n",
       "         -1.1662e+00,  7.1753e-01, -1.2947e+00,  6.8860e-01, -1.0975e+00,\n",
       "          2.3920e-01,  1.2884e+00, -1.4832e-01,  5.4393e-01, -5.3623e-01,\n",
       "         -9.6472e-01,  1.4942e+00, -1.6468e+00, -6.9458e-01,  9.9796e-01],\n",
       "        [-3.8609e-01,  1.2954e+00, -1.1917e+00, -2.9049e-01, -2.7328e-01,\n",
       "         -8.1280e-01,  6.7841e-01, -9.6358e-01, -1.4520e+00,  4.1407e-01,\n",
       "          1.3722e+00,  1.0524e-01, -7.6150e-01,  1.2587e+00,  5.5831e-01,\n",
       "          4.3195e-02, -6.4020e-01, -3.0788e-01, -1.1583e+00, -1.5405e+00,\n",
       "          9.2405e-01, -4.1511e-01,  9.1223e-01,  1.4843e+00,  1.0580e-01,\n",
       "         -1.5426e+00, -1.9612e-01,  1.3202e-01,  8.7597e-01,  7.3102e-01,\n",
       "          1.9807e-01, -4.9958e-01, -1.3811e-01,  7.9969e-01, -8.9153e-01,\n",
       "          2.7409e+00, -9.6079e-01,  1.0243e+00, -1.3142e+00, -2.1257e-01,\n",
       "         -4.2768e-01, -4.5790e-01,  1.6724e-01,  3.6183e-01,  1.1749e+00,\n",
       "          7.6137e-01, -1.4779e+00,  5.2981e-01, -2.2433e+00, -3.6222e-01],\n",
       "        [-4.7245e-01,  3.0727e-01, -2.8654e+00,  1.9283e+00,  1.1026e+00,\n",
       "         -8.2235e-01, -5.3952e-01, -1.0726e+00, -8.0313e-01,  1.9129e-01,\n",
       "         -1.8985e+00,  1.4300e+00,  2.5515e-01,  3.6384e-01,  6.7899e-01,\n",
       "          1.0046e+00,  4.9313e-01,  8.3323e-01,  2.3201e-01,  5.9148e-01,\n",
       "         -3.9001e-01, -1.3969e+00,  2.9361e-01, -1.3197e+00,  1.1040e+00,\n",
       "          3.8289e-01,  1.4647e-02,  1.2844e+00, -7.8565e-01,  1.0179e+00,\n",
       "          5.5199e-01,  2.0300e+00,  1.0637e+00,  3.8627e-01, -1.1232e+00,\n",
       "          2.6841e-02, -7.8363e-01, -1.8048e-01,  3.7761e-01,  1.8840e+00,\n",
       "         -1.9820e-01, -2.9389e-02,  1.8403e-01, -5.6014e-01,  8.9602e-01,\n",
       "         -8.1524e-02, -5.9504e-01,  1.6390e-01, -7.3822e-02,  1.4172e-01],\n",
       "        [ 1.2489e-01,  1.2291e+00,  1.5361e-01, -3.8249e-01,  1.3490e+00,\n",
       "         -7.0299e-02,  1.3607e+00, -1.6080e+00,  1.3999e+00,  1.5656e-01,\n",
       "         -1.7330e+00, -1.5988e+00, -1.1314e+00, -1.7181e+00,  3.7964e-01,\n",
       "         -1.1114e-01, -6.3523e-01, -6.3899e-01,  7.5921e-01,  6.9444e-01,\n",
       "         -9.4574e-01,  7.6824e-01,  1.1433e+00, -6.5999e-01, -1.9366e+00,\n",
       "          2.6739e-01,  3.2431e-01,  7.3170e-01, -2.0700e-01, -7.9992e-01,\n",
       "         -8.4144e-01, -5.5670e-01, -1.1460e+00,  5.1458e-01, -5.1155e-01,\n",
       "         -1.6473e-01, -8.9112e-01,  1.5147e+00, -7.5162e-01, -2.2160e+00,\n",
       "         -7.4768e-01, -5.3853e-02, -2.0128e+00, -8.6175e-01,  3.4168e-01,\n",
       "         -2.0374e+00, -3.4678e-01, -3.7028e-01,  1.0754e+00, -7.7528e-01],\n",
       "        [ 1.0872e-01, -2.4615e-01,  1.8178e+00,  4.0906e-01,  2.2287e+00,\n",
       "          3.5666e-01, -1.4283e-01,  1.9312e-02,  1.7834e+00, -1.8846e-01,\n",
       "          1.2991e+00, -3.2402e-01, -8.2382e-01, -1.2780e+00, -2.5058e-01,\n",
       "          6.1833e-01,  1.5866e+00,  9.8249e-02, -9.6631e-02,  1.0472e+00,\n",
       "         -4.5879e-01, -8.1705e-01, -8.5324e-01,  2.1772e+00,  2.7812e-01,\n",
       "         -1.3568e+00,  2.6808e-01,  2.3820e-01, -1.0735e+00, -1.8583e-02,\n",
       "         -8.7472e-01,  1.5070e+00,  1.3637e+00, -2.3807e-01, -5.2098e-01,\n",
       "          1.1880e+00, -1.7222e+00, -6.5299e-01,  1.1987e+00,  4.2818e-01,\n",
       "         -1.7833e+00, -2.6301e-01, -8.7562e-01, -2.3725e-01, -2.6143e-02,\n",
       "          1.5182e-01, -1.4659e+00,  1.3059e+00, -3.2178e-02,  9.9724e-03]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = x(dataset[0][0])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9f34ff3-2a95-4ae4-8ed6-6bb04814d490",
   "metadata": {},
   "outputs": [],
   "source": [
    " y = nn.RNN(50 , 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5268e84f-eeca-4685-9b93-e95dce3a344f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4502, -0.4295,  0.3240, -0.4273, -0.3014, -0.5953, -0.1974,  0.1112,\n",
       "           0.6696, -0.3774, -0.1458,  0.0313, -0.5461,  0.3412,  0.1190, -0.6218,\n",
       "          -0.6943,  0.3040, -0.4488, -0.3985, -0.4769, -0.0148,  0.0476,  0.0398,\n",
       "           0.1206,  0.0245,  0.2245, -0.5463, -0.5081, -0.7515,  0.6875, -0.2168,\n",
       "          -0.4771, -0.3542,  0.2023,  0.3320,  0.6957,  0.3494,  0.7697,  0.4478,\n",
       "          -0.2752, -0.3144, -0.4444,  0.3991,  0.1023, -0.0470,  0.4776,  0.4911,\n",
       "           0.0946, -0.4103, -0.3804,  0.0035,  0.3420,  0.7257,  0.2220,  0.0363,\n",
       "          -0.5749, -0.1331, -0.2146,  0.0432, -0.3537, -0.1544, -0.8328, -0.0790],\n",
       "         [-0.5564,  0.2084, -0.7508, -0.2657,  0.2320, -0.7379, -0.1366,  0.7019,\n",
       "          -0.3935,  0.6444,  0.4816,  0.4393, -0.1858,  0.2224,  0.0222, -0.0561,\n",
       "           0.0157,  0.4280, -0.0966, -0.5466,  0.5270, -0.3998, -0.1918,  0.2335,\n",
       "          -0.4434,  0.4260, -0.2151, -0.3599,  0.1738, -0.5136, -0.0836, -0.5972,\n",
       "          -0.0648, -0.5261, -0.3593, -0.3048, -0.6996, -0.4155, -0.2020, -0.6351,\n",
       "           0.1389,  0.6576, -0.2242, -0.6990, -0.2780,  0.3635,  0.3006,  0.1348,\n",
       "           0.0169, -0.7598,  0.4880, -0.4750, -0.2121,  0.2070,  0.4588,  0.2665,\n",
       "          -0.3722, -0.2882,  0.6096,  0.3304, -0.2080,  0.5013,  0.8669,  0.1170],\n",
       "         [ 0.2565, -0.1392, -0.0102, -0.6394, -0.0284, -0.2874, -0.1658, -0.1696,\n",
       "          -0.3955,  0.1222, -0.1877,  0.5664,  0.6507,  0.2079,  0.4372, -0.7609,\n",
       "           0.2432,  0.8236,  0.6352,  0.1035, -0.1219,  0.8298,  0.1666, -0.0258,\n",
       "           0.5471, -0.3922,  0.7905,  0.5518,  0.0487,  0.5887, -0.0891,  0.5026,\n",
       "          -0.6430, -0.1840, -0.7222, -0.1595,  0.1521, -0.1649,  0.1114, -0.5769,\n",
       "           0.0902,  0.1564, -0.6950,  0.2837, -0.2015, -0.5762,  0.1509, -0.5160,\n",
       "          -0.2080, -0.1409, -0.8308, -0.2014,  0.1568, -0.0110, -0.5480,  0.1071,\n",
       "           0.4473, -0.7689, -0.4898,  0.5582,  0.5342,  0.3645,  0.2966,  0.5388],\n",
       "         [ 0.2903, -0.2232, -0.5164, -0.1275, -0.4934,  0.7779,  0.6415, -0.7473,\n",
       "          -0.6340,  0.6165,  0.6748, -0.7271,  0.3091, -0.3165,  0.2860, -0.2393,\n",
       "           0.7936,  0.0899,  0.8840, -0.0464,  0.4551, -0.2329,  0.5415, -0.6058,\n",
       "           0.6618, -0.5460, -0.2745,  0.5154,  0.5678, -0.4736, -0.4063, -0.3673,\n",
       "           0.2957, -0.7921,  0.0805, -0.1000, -0.1390,  0.7933, -0.0230,  0.5583,\n",
       "          -0.3286,  0.7733,  0.4479,  0.3860, -0.2479, -0.5262,  0.1563, -0.5514,\n",
       "          -0.2732, -0.1997, -0.5838, -0.2544,  0.4043,  0.0930, -0.2829, -0.1879,\n",
       "          -0.8903,  0.0724,  0.7113,  0.3723,  0.5613, -0.0185, -0.6085,  0.0920],\n",
       "         [ 0.5133,  0.2535,  0.0084,  0.2332, -0.6595,  0.3049,  0.8319,  0.3162,\n",
       "          -0.3197,  0.2533, -0.5830, -0.1424,  0.1216,  0.3901,  0.1653, -0.2256,\n",
       "          -0.2212,  0.7469,  0.7196, -0.4087, -0.8810,  0.1733, -0.4781,  0.6447,\n",
       "           0.5272,  0.2252, -0.6121, -0.3112, -0.4517,  0.0651,  0.2561, -0.3741,\n",
       "          -0.6396, -0.5595,  0.2591, -0.3762,  0.1031,  0.8892, -0.1088,  0.0855,\n",
       "           0.6289, -0.6658, -0.7848,  0.3630,  0.3813,  0.1887,  0.3450, -0.0746,\n",
       "          -0.4749, -0.3861, -0.6996,  0.1950, -0.2006, -0.6507, -0.8036,  0.4811,\n",
       "          -0.2290, -0.6211,  0.1466, -0.9080, -0.0465, -0.0339, -0.7344, -0.5300],\n",
       "         [-0.6081, -0.4009,  0.2240, -0.5875,  0.2071, -0.0590,  0.6264, -0.4862,\n",
       "          -0.0218,  0.3950, -0.5908,  0.5934, -0.6648, -0.3847, -0.7076,  0.3145,\n",
       "           0.3555,  0.0317, -0.5675,  0.5596, -0.1512,  0.5378,  0.5064,  0.0937,\n",
       "           0.3413, -0.7322,  0.0637,  0.1429,  0.1414, -0.5315,  0.3515, -0.4117,\n",
       "           0.4547,  0.0659, -0.0349,  0.6517,  0.3695,  0.3873, -0.2892, -0.4244,\n",
       "          -0.2136, -0.1682, -0.7800,  0.0472,  0.5855,  0.1156,  0.3151, -0.2608,\n",
       "          -0.3349,  0.1780,  0.6155, -0.8439, -0.3380,  0.2947,  0.1465,  0.8130,\n",
       "          -0.2238,  0.0394,  0.7373,  0.7920,  0.7627,  0.5654, -0.5597, -0.1181]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.6081, -0.4009,  0.2240, -0.5875,  0.2071, -0.0590,  0.6264, -0.4862,\n",
       "          -0.0218,  0.3950, -0.5908,  0.5934, -0.6648, -0.3847, -0.7076,  0.3145,\n",
       "           0.3555,  0.0317, -0.5675,  0.5596, -0.1512,  0.5378,  0.5064,  0.0937,\n",
       "           0.3413, -0.7322,  0.0637,  0.1429,  0.1414, -0.5315,  0.3515, -0.4117,\n",
       "           0.4547,  0.0659, -0.0349,  0.6517,  0.3695,  0.3873, -0.2892, -0.4244,\n",
       "          -0.2136, -0.1682, -0.7800,  0.0472,  0.5855,  0.1156,  0.3151, -0.2608,\n",
       "          -0.3349,  0.1780,  0.6155, -0.8439, -0.3380,  0.2947,  0.1465,  0.8130,\n",
       "          -0.2238,  0.0394,  0.7373,  0.7920,  0.7627,  0.5654, -0.5597, -0.1181]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(a) # tuple ke andar 2D tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45982e3d-012c-451d-bb74-58e5adb6358e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 64])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(a)[0].shape  # hidden state output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f68ba295-fc22-4b41-b8ef-a20a95eb64ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = y(a)[1].shape  #final state output stored in b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e03509c6-690a-44dd-a3e2-570ee5f1bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f58d2ce-a45f-40b5-b3a9-fbc610428e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b86d8ecc-5199-400e-b4d2-50f23f47dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e49ef1d9-9b3c-427c-9990-0ee7d8585772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 10.719632\n",
      "Epoch: 2, Loss: 9.530342\n",
      "Epoch: 3, Loss: 8.322802\n",
      "Epoch: 4, Loss: 7.495620\n",
      "Epoch: 5, Loss: 6.774483\n",
      "Epoch: 6, Loss: 6.005592\n",
      "Epoch: 7, Loss: 5.424946\n",
      "Epoch: 8, Loss: 4.887863\n",
      "Epoch: 9, Loss: 4.451186\n",
      "Epoch: 10, Loss: 4.067106\n",
      "Epoch: 11, Loss: 3.691444\n",
      "Epoch: 12, Loss: 3.418797\n",
      "Epoch: 13, Loss: 3.140523\n",
      "Epoch: 14, Loss: 2.897867\n",
      "Epoch: 15, Loss: 2.681110\n",
      "Epoch: 16, Loss: 2.478679\n",
      "Epoch: 17, Loss: 2.299879\n",
      "Epoch: 18, Loss: 2.142529\n",
      "Epoch: 19, Loss: 1.998504\n",
      "Epoch: 20, Loss: 1.864453\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(epochs):  # 20 times the loop will get executed\n",
    "\n",
    "  total_loss = 0  # for measuring loss\n",
    "\n",
    "  for question, answer in dataloader:  \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    output = model(question)\n",
    "\n",
    "    # loss -> output shape (1,324) - (1)\n",
    "    loss = criterion(output, answer[0])\n",
    "\n",
    "    # gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "  print(f\"Epoch: {epoch+1}, Loss: {total_loss:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9af01b0-bfe1-4ac0-a048-a564b70679df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, question, threshold=0.5):\n",
    "\n",
    "  # convert question to numbers\n",
    "  numerical_question = text_to_indices(question, vocab)\n",
    "\n",
    "  # tensor\n",
    "  question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
    "\n",
    "  # send to model\n",
    "  output = model(question_tensor)\n",
    "\n",
    "  # convert logits to probs\n",
    "  probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "  # find index of max prob\n",
    "  value, index = torch.max(probs, dim=1)\n",
    "\n",
    "  if value < threshold:\n",
    "    print(\"I don't know\")\n",
    "\n",
    "  print(list(vocab.keys())[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c165b07-0a0c-4abe-8fe2-fe41ecc36c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leonardo-da-vinci\n"
     ]
    }
   ],
   "source": [
    "predict(model, \"Who painted the Mona Lisa?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e309a0c2-5fe8-4f96-8640-324c4f769d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.keys())[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd0e93-e7c1-4bfb-8d78-927f7ec25a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
